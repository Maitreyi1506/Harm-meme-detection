{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2023-11-02T15:53:41.005467Z","iopub.execute_input":"2023-11-02T15:53:41.005786Z","iopub.status.idle":"2023-11-02T15:53:49.803124Z","shell.execute_reply.started":"2023-11-02T15:53:41.005758Z","shell.execute_reply":"2023-11-02T15:53:49.802047Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/PetrochukM/PyTorch-NLP.git","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:53:49.805053Z","iopub.execute_input":"2023-11-02T15:53:49.805866Z","iopub.status.idle":"2023-11-02T15:54:07.057783Z","shell.execute_reply.started":"2023-11-02T15:53:49.805829Z","shell.execute_reply":"2023-11-02T15:54:07.056671Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/PetrochukM/PyTorch-NLP.git\n  Cloning https://github.com/PetrochukM/PyTorch-NLP.git to /tmp/pip-req-build-llm6k4rp\n  Running command git clone --filter=blob:none --quiet https://github.com/PetrochukM/PyTorch-NLP.git /tmp/pip-req-build-llm6k4rp\n  Resolved https://github.com/PetrochukM/PyTorch-NLP.git to commit 53d7edcb8e0c099efce7c2ddf8cd7c44157fcac3\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-nlp==0.5.0) (1.23.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-nlp==0.5.0) (4.66.1)\nBuilding wheels for collected packages: pytorch-nlp\n  Building wheel for pytorch-nlp (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-nlp: filename=pytorch_nlp-0.5.0-py3-none-any.whl size=88719 sha256=0131054286e1394da92dd5ae1b14fd8f4b57e8eb39978a84005e21f7fe3a56b1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-qzsasg8m/wheels/a5/93/b0/9f0138afb1271281613a5af71272c5b246fdd2d421c6fbdf88\nSuccessfully built pytorch-nlp\nInstalling collected packages: pytorch-nlp\nSuccessfully installed pytorch-nlp-0.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch \nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\nfrom torchnlp import encoders\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.metrics import hamming_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import  mean_absolute_error\nfrom sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\nfrom pathlib import Path\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:07.059341Z","iopub.execute_input":"2023-11-02T15:54:07.059733Z","iopub.status.idle":"2023-11-02T15:54:11.285423Z","shell.execute_reply.started":"2023-11-02T15:54:07.059680Z","shell.execute_reply":"2023-11-02T15:54:11.284478Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:11.287862Z","iopub.execute_input":"2023-11-02T15:54:11.288297Z","iopub.status.idle":"2023-11-02T15:54:11.292779Z","shell.execute_reply.started":"2023-11-02T15:54:11.288270Z","shell.execute_reply":"2023-11-02T15:54:11.291369Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load the ROI features (Covid)\ntrain_ROI = torch.load(\"/kaggle/input/harmeme/MINI_PROJECT_2/harmeme_saved_feat_ROIENT/harmeme_ROI_MOMENTA/cov/memes_harmfulness/harmeme_cov_train_ROI.pt\")\nval_ROI = torch.load(\"/kaggle/input/harmeme/MINI_PROJECT_2/harmeme_saved_feat_ROIENT/harmeme_ROI_MOMENTA/cov/memes_harmfulness/harmeme_cov_val_ROI.pt\")\ntest_ROI = torch.load(\"/kaggle/input/harmeme/MINI_PROJECT_2/harmeme_saved_feat_ROIENT/harmeme_ROI_MOMENTA/cov/memes_harmfulness/harmeme_cov_test_ROI.pt\")\n# Load the ENT features\ntrain_ENT = torch.load(\"/kaggle/input/albert-text-features/Train_text_Drob_Feature_albert.pt\")\nval_ENT = torch.load(\"/kaggle/input/albert-text-features/Val_text_Drob_Feature_albert.pt\")\ntest_ENT = torch.load(\"/kaggle/input/albert-text-features/Test_text_Drob_Feature_albert.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:11.294447Z","iopub.execute_input":"2023-11-02T15:54:11.295417Z","iopub.status.idle":"2023-11-02T15:54:15.842813Z","shell.execute_reply.started":"2023-11-02T15:54:11.295380Z","shell.execute_reply":"2023-11-02T15:54:15.841989Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_dir_cov = \"/kaggle/input/harmeme/MINI_PROJECT_2/Harm-C/datasets/memes/defaults/images\"\ntrain_path_cov = \"/kaggle/input/d/yashnikam10/supplementary-data/final_train.json\"\ndev_path_cov   = \"/kaggle/input/d/yashnikam10/supplementary-data/final_val_dict.json\"\ntest_path_cov  = \"/kaggle/input/d/yashnikam10/supplementary-data/final_test.json\"","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:15.843978Z","iopub.execute_input":"2023-11-02T15:54:15.844267Z","iopub.status.idle":"2023-11-02T15:54:15.849605Z","shell.execute_reply.started":"2023-11-02T15:54:15.844242Z","shell.execute_reply":"2023-11-02T15:54:15.848508Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_samples_frame = pd.read_json(train_path_cov, lines=True)\ntrain_samples_frame.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:15.850650Z","iopub.execute_input":"2023-11-02T15:54:15.850947Z","iopub.status.idle":"2023-11-02T15:54:16.043297Z","shell.execute_reply.started":"2023-11-02T15:54:15.850901Z","shell.execute_reply":"2023-11-02T15:54:16.042366Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                id                image                            labels  \\\n0   covid_memes_18   covid_memes_18.png    [somewhat harmful, individual]   \n1   covid_memes_19   covid_memes_19.png  [somewhat harmful, organization]   \n2  covid_memes_252  covid_memes_252.png                     [not harmful]   \n3  covid_memes_255  covid_memes_255.png                     [not harmful]   \n4   covid_memes_20   covid_memes_20.png    [somewhat harmful, individual]   \n\n                                                text  \\\n0  Bernie or Elizabeth?\\nBe informed.Compare them...   \n1  Extending the\\nBrexit deadline until\\nOctober ...   \n2  kwai\\ngkwa 0964\\n#nnevvy\\napplause to Thais fr...   \n3  So, I order this\\nfoce mask to\\nprotect ogains...   \n4  best candidate for\\nJA\\n2020\\njoe biden\\nKamal...   \n\n                                             bb_dict  \n0  [[0.6680887372013651, 0.632857142857142, 0.825...  \n1                                                 []  \n2  [[0.522067189216613, 0.207160174846649, 0.4088...  \n3                                                 []  \n4  [[0.47865853658536506, 0.32835820895522305, 0....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>image</th>\n      <th>labels</th>\n      <th>text</th>\n      <th>bb_dict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>covid_memes_18</td>\n      <td>covid_memes_18.png</td>\n      <td>[somewhat harmful, individual]</td>\n      <td>Bernie or Elizabeth?\\nBe informed.Compare them...</td>\n      <td>[[0.6680887372013651, 0.632857142857142, 0.825...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>covid_memes_19</td>\n      <td>covid_memes_19.png</td>\n      <td>[somewhat harmful, organization]</td>\n      <td>Extending the\\nBrexit deadline until\\nOctober ...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>covid_memes_252</td>\n      <td>covid_memes_252.png</td>\n      <td>[not harmful]</td>\n      <td>kwai\\ngkwa 0964\\n#nnevvy\\napplause to Thais fr...</td>\n      <td>[[0.522067189216613, 0.207160174846649, 0.4088...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>covid_memes_255</td>\n      <td>covid_memes_255.png</td>\n      <td>[not harmful]</td>\n      <td>So, I order this\\nfoce mask to\\nprotect ogains...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>covid_memes_20</td>\n      <td>covid_memes_20.png</td>\n      <td>[somewhat harmful, individual]</td>\n      <td>best candidate for\\nJA\\n2020\\njoe biden\\nKamal...</td>\n      <td>[[0.47865853658536506, 0.32835820895522305, 0....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_samples_frame = pd.read_json(test_path_cov, lines=True)\ntest_samples_frame.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:16.044228Z","iopub.execute_input":"2023-11-02T15:54:16.044467Z","iopub.status.idle":"2023-11-02T15:54:16.075339Z","shell.execute_reply.started":"2023-11-02T15:54:16.044444Z","shell.execute_reply":"2023-11-02T15:54:16.074483Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                 id                 image         labels  \\\n0  covid_memes_5425  covid_memes_5425.png  [not harmful]   \n1  covid_memes_5426  covid_memes_5426.png  [not harmful]   \n2  covid_memes_5429  covid_memes_5429.png  [not harmful]   \n3  covid_memes_5430  covid_memes_5430.png  [not harmful]   \n4  covid_memes_5434  covid_memes_5434.png  [not harmful]   \n\n                                                text  \\\n0  gwen\\n@gwenervi\\ndis gon be trump tomorrow aft...   \n1  Armani\\n@historyofarmani\\nBiden after hearing ...   \n2  MESSAGE FROM TRUMP TO\\nCOVID-19\\nLEAVE NOW OR ...   \n3  COVID-19 STARTED DURING HIS TERM\\nSOIT SHOULD ...   \n4  TRUMPS RESPONSE TO COVID-19\\nUMP\\nTAYM\\nINCLUD...   \n\n                                             bb_dict  \n0                                                 []  \n1  [[0.071942446043165, 0.2734375, 0.393884892086...  \n2  [[0.41454545454545405, 0.125683060109289, 0.64...  \n3  [[0.257246376811594, 0.19125683060109203, 0.54...  \n4  [[0.25703868269920305, 0.167230486869812, 0.49...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>image</th>\n      <th>labels</th>\n      <th>text</th>\n      <th>bb_dict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>covid_memes_5425</td>\n      <td>covid_memes_5425.png</td>\n      <td>[not harmful]</td>\n      <td>gwen\\n@gwenervi\\ndis gon be trump tomorrow aft...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>covid_memes_5426</td>\n      <td>covid_memes_5426.png</td>\n      <td>[not harmful]</td>\n      <td>Armani\\n@historyofarmani\\nBiden after hearing ...</td>\n      <td>[[0.071942446043165, 0.2734375, 0.393884892086...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>covid_memes_5429</td>\n      <td>covid_memes_5429.png</td>\n      <td>[not harmful]</td>\n      <td>MESSAGE FROM TRUMP TO\\nCOVID-19\\nLEAVE NOW OR ...</td>\n      <td>[[0.41454545454545405, 0.125683060109289, 0.64...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>covid_memes_5430</td>\n      <td>covid_memes_5430.png</td>\n      <td>[not harmful]</td>\n      <td>COVID-19 STARTED DURING HIS TERM\\nSOIT SHOULD ...</td>\n      <td>[[0.257246376811594, 0.19125683060109203, 0.54...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>covid_memes_5434</td>\n      <td>covid_memes_5434.png</td>\n      <td>[not harmful]</td>\n      <td>TRUMPS RESPONSE TO COVID-19\\nUMP\\nTAYM\\nINCLUD...</td>\n      <td>[[0.25703868269920305, 0.167230486869812, 0.49...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install ftfy","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:16.076576Z","iopub.execute_input":"2023-11-02T15:54:16.077217Z","iopub.status.idle":"2023-11-02T15:54:28.082463Z","shell.execute_reply.started":"2023-11-02T15:54:16.077179Z","shell.execute_reply":"2023-11-02T15:54:28.081464Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting ftfy\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.6)\nInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import gzip\nimport html\nimport os\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:28.086858Z","iopub.execute_input":"2023-11-02T15:54:28.087140Z","iopub.status.idle":"2023-11-02T15:54:28.180637Z","shell.execute_reply.started":"2023-11-02T15:54:28.087116Z","shell.execute_reply":"2023-11-02T15:54:28.179674Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!wget -O bpe_simple_vocab_16e6.txt.gz https://drive.google.com/uc?id=1YNkhStUg9YJ3WPUeCWhHyyMUmVqlWrG5","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:28.181687Z","iopub.execute_input":"2023-11-02T15:54:28.181994Z","iopub.status.idle":"2023-11-02T15:54:31.082219Z","shell.execute_reply.started":"2023-11-02T15:54:28.181963Z","shell.execute_reply":"2023-11-02T15:54:31.081214Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"--2023-11-02 15:54:29--  https://drive.google.com/uc?id=1YNkhStUg9YJ3WPUeCWhHyyMUmVqlWrG5\nResolving drive.google.com (drive.google.com)... 74.125.126.101, 74.125.126.139, 74.125.126.100, ...\nConnecting to drive.google.com (drive.google.com)|74.125.126.101|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://doc-0s-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/k4u6j127qmnfhp14iukbiimdnmeatlo8/1698940425000/14204125426768851410/*/1YNkhStUg9YJ3WPUeCWhHyyMUmVqlWrG5?uuid=4cfa1a1d-c97b-4042-b1b4-c39db54a9102 [following]\nWarning: wildcards not supported in HTTP.\n--2023-11-02 15:54:30--  https://doc-0s-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/k4u6j127qmnfhp14iukbiimdnmeatlo8/1698940425000/14204125426768851410/*/1YNkhStUg9YJ3WPUeCWhHyyMUmVqlWrG5?uuid=4cfa1a1d-c97b-4042-b1b4-c39db54a9102\nResolving doc-0s-a8-docs.googleusercontent.com (doc-0s-a8-docs.googleusercontent.com)... 173.194.74.132, 2607:f8b0:4001:c0d::84\nConnecting to doc-0s-a8-docs.googleusercontent.com (doc-0s-a8-docs.googleusercontent.com)|173.194.74.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1356917 (1.3M) [application/x-gzip]\nSaving to: ‘bpe_simple_vocab_16e6.txt.gz’\n\nbpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.01s   \n\n2023-11-02 15:54:30 (119 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = \"/kaggle/working/bpe_simple_vocab_16e6.txt.gz\"):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n#         with gzip.open(bpe_path,'w') as t:\n#        with gzip.open(bpe_path,'rt', encoding=\"utf-8\") as f:\n#                merges = f.read()\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n        #print(merges)\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        #print(vocab)\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+'</w>'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:31.083719Z","iopub.execute_input":"2023-11-02T15:54:31.084116Z","iopub.status.idle":"2023-11-02T15:54:31.111773Z","shell.execute_reply.started":"2023-11-02T15:54:31.084088Z","shell.execute_reply":"2023-11-02T15:54:31.110872Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Downloadig the clip model\nMODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",    \n}\n! wget {MODELS[\"ViT-B/32\"]} -O clip_model.pt","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:31.112970Z","iopub.execute_input":"2023-11-02T15:54:31.113255Z","iopub.status.idle":"2023-11-02T15:54:33.883333Z","shell.execute_reply.started":"2023-11-02T15:54:31.113230Z","shell.execute_reply":"2023-11-02T15:54:33.882401Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"--2023-11-02 15:54:32--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\nResolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.38, 13.107.213.38, 2620:1ec:bdf::38, ...\nConnecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.38|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 353976522 (338M) [application/octet-stream]\nSaving to: ‘clip_model.pt’\n\nclip_model.pt       100%[===================>] 337.58M   210MB/s    in 1.6s    \n\n2023-11-02 15:54:33 (210 MB/s) - ‘clip_model.pt’ saved [353976522/353976522]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"clip_model = torch.jit.load(\"clip_model.pt\").cuda().eval()\ninput_resolution = clip_model.input_resolution.item()\ncontext_length = clip_model.context_length.item()\nvocab_size = clip_model.vocab_size.item()\n\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:33.885199Z","iopub.execute_input":"2023-11-02T15:54:33.885662Z","iopub.status.idle":"2023-11-02T15:54:34.488501Z","shell.execute_reply.started":"2023-11-02T15:54:33.885604Z","shell.execute_reply":"2023-11-02T15:54:34.487522Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model parameters: 151,277,313\nInput resolution: 224\nContext length: 77\nVocab size: 49408\n","output_type":"stream"}]},{"cell_type":"code","source":"preprocess = Compose([\n    Resize(input_resolution, interpolation=Image.BICUBIC),\n    CenterCrop(input_resolution),\n    ToTensor()\n    ])\ntokenizer = SimpleTokenizer()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:34.489804Z","iopub.execute_input":"2023-11-02T15:54:34.490203Z","iopub.status.idle":"2023-11-02T15:54:34.612938Z","shell.execute_reply.started":"2023-11-02T15:54:34.490164Z","shell.execute_reply":"2023-11-02T15:54:34.611841Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def process_image_clip(in_img):\n    image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n    image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n    \n    image = preprocess(Image.open(in_img).convert(\"RGB\"))\n    \n    image_input = torch.tensor(np.stack(image)).cuda()\n    image_input -= image_mean[:, None, None]\n    image_input /= image_std[:, None, None]\n    return image_input","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:34.614363Z","iopub.execute_input":"2023-11-02T15:54:34.615013Z","iopub.status.idle":"2023-11-02T15:54:34.621834Z","shell.execute_reply.started":"2023-11-02T15:54:34.614977Z","shell.execute_reply":"2023-11-02T15:54:34.620751Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def process_text_clip(in_text):    \n    text_token = tokenizer.encode(in_text)\n    text_input = torch.zeros(clip_model.context_length, dtype=torch.long)\n    sot_token = tokenizer.encoder['<|startoftext|>']\n    eot_token = tokenizer.encoder['<|endoftext|>']\n    tokens = [sot_token] + text_token[:75] + [eot_token]\n    text_input[:len(tokens)] = torch.tensor(tokens)\n    text_input = text_input.cuda()\n    return text_input","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:34.623056Z","iopub.execute_input":"2023-11-02T15:54:34.623335Z","iopub.status.idle":"2023-11-02T15:54:34.637770Z","shell.execute_reply.started":"2023-11-02T15:54:34.623311Z","shell.execute_reply":"2023-11-02T15:54:34.636890Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import optim, nn\nfrom torchvision import models, transforms\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:34.638844Z","iopub.execute_input":"2023-11-02T15:54:34.639078Z","iopub.status.idle":"2023-11-02T15:54:34.865874Z","shell.execute_reply.started":"2023-11-02T15:54:34.639057Z","shell.execute_reply":"2023-11-02T15:54:34.864929Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# YOLOv8","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:34.867157Z","iopub.execute_input":"2023-11-02T15:54:34.867503Z","iopub.status.idle":"2023-11-02T15:54:48.120329Z","shell.execute_reply.started":"2023-11-02T15:54:34.867470Z","shell.execute_reply":"2023-11-02T15:54:48.118936Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.0.203-py3-none-any.whl (644 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: numpy>=1.22.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.23.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.8.0.76)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.2)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.0.0)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.15.1)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.0.2)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nCollecting thop>=0.1.1 (from ultralytics)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nInstalling collected packages: thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.0.203\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics.nn.tasks import DetectionModel\nclass YOLOv8DetectionAndFeatureExtractorModel(DetectionModel):\n    def __init__(self, cfg='yolov8n.yaml', ch=3, nc=None, verbose=True):  # model, input channels, number of classes\n        super().__init__(cfg, ch, nc, verbose)\n    \n    def custom_forward(self, x):\n        \"\"\"\n        This is a modified version of the original _forward_once() method in BaseModel,\n        found in ultralytics/nn/tasks.py.\n        The original method returns only the detection output, while this method returns\n        both the detection output and the features extracted by the last convolutional layer.\n        \"\"\"\n        y = []\n        features = None\n        for m in self.model:\n            if m.f != -1:  # if not from previous layer\n                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n            if torch.is_tensor(x):\n                features = x # keep the last tensor as features\n            x = m(x)  # run\n            if torch.is_tensor(x):\n                features = x # keep the last tensor as features\n            y.append(x if m.i in self.save else None)  # save output\n        if torch.is_tensor(x):\n            features = x # keep the last tensor as features\n        return features, x # return features and detection output\n\ndef create_yolov8_model(model_name_or_path,nc):\n    from ultralytics.nn.tasks import attempt_load_one_weight\n    #from ultralytics.yolo.cfg import get_cfg\n    ckpt = None\n    if str(model_name_or_path).endswith('.pt'):\n        weights, ckpt = attempt_load_one_weight(model_name_or_path)\n        cfg = ckpt['model'].yaml\n    else:\n        cfg = model_name_or_path\n    model = YOLOv8DetectionAndFeatureExtractorModel(cfg, nc=nc, verbose=True)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:48.122098Z","iopub.execute_input":"2023-11-02T15:54:48.122402Z","iopub.status.idle":"2023-11-02T15:54:48.329016Z","shell.execute_reply.started":"2023-11-02T15:54:48.122375Z","shell.execute_reply":"2023-11-02T15:54:48.327920Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model_yolov8 = create_yolov8_model('yolov8n.pt', 80)\n\n# Change the device to GPU\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\nmodel_yolov8 = model_yolov8.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:48.330686Z","iopub.execute_input":"2023-11-02T15:54:48.331002Z","iopub.status.idle":"2023-11-02T15:54:49.485379Z","shell.execute_reply.started":"2023-11-02T15:54:48.330974Z","shell.execute_reply":"2023-11-02T15:54:49.484280Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n100%|██████████| 6.23M/6.23M [00:00<00:00, 74.1MB/s]\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \nModel summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.486708Z","iopub.execute_input":"2023-11-02T15:54:49.487021Z","iopub.status.idle":"2023-11-02T15:54:49.491410Z","shell.execute_reply.started":"2023-11-02T15:54:49.486992Z","shell.execute_reply":"2023-11-02T15:54:49.490342Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"transform_yolov8_BB = transforms.Compose([\n  transforms.ToPILImage(),\n#   transforms.CenterCrop(512),\n  transforms.Resize((640,640)),\n  transforms.ToTensor()                              \n])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.492873Z","iopub.execute_input":"2023-11-02T15:54:49.493228Z","iopub.status.idle":"2023-11-02T15:54:49.507662Z","shell.execute_reply.started":"2023-11-02T15:54:49.493199Z","shell.execute_reply":"2023-11-02T15:54:49.506681Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\n\n# Transform the image, so it becomes readable with the model\ntransform_yolov8_center = transforms.Compose([\n  transforms.ToPILImage(),\n  transforms.CenterCrop(640),\n  transforms.Resize(640),\n  transforms.ToTensor()                              \n])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.509015Z","iopub.execute_input":"2023-11-02T15:54:49.509361Z","iopub.status.idle":"2023-11-02T15:54:49.518167Z","shell.execute_reply.started":"2023-11-02T15:54:49.509326Z","shell.execute_reply":"2023-11-02T15:54:49.516953Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def get_image_yolov8_BB(l, t, r, b, in_im): \n#     left, top, right, bottom and input image\n    img = cv2.imread(in_im)\n    h, w, _ = img.shape\n    # crop\n    x1 = int(np.floor(l*w))\n    x2 = int(np.floor(r*w))\n    y1 = int(np.floor(b*h))\n    y2 = int(np.floor(t*h))\n    crop_img = img[y1:y2, x1:x2]    \n    \n    # Transform the cropped image\n    img = transform_yolov8_BB(crop_img)\n    # Reshape the image. PyTorch model reads 4-dimensional tensor\n    # [batch_size, channels, width, height]\n    img = img.reshape(1, 3, 640, 640)\n    img = img.to(device)\n    # We only extract features, so we don't need gradient\n    with torch.no_grad():\n        # Extract the feature from the image\n        feature = model_yolov8.custom_forward(img)[0].flatten()\n    # Convert to NumPy Array, Reshape it, and save it to features variable\n    return feature","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.519576Z","iopub.execute_input":"2023-11-02T15:54:49.519994Z","iopub.status.idle":"2023-11-02T15:54:49.530406Z","shell.execute_reply.started":"2023-11-02T15:54:49.519952Z","shell.execute_reply":"2023-11-02T15:54:49.529444Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def get_image_yolov8_center(in_im):\n    # Set the image path\n    path = in_im\n    # Read the file\n    img = cv2.imread(path)\n    # Transform the image\n    img = transform_yolov8_center(img)\n    # Reshape the image. PyTorch model reads 4-dimensional tensor\n    # [batch_size, channels, width, height]\n    img = img.reshape(1, 3, 640, 640)\n    img = img.to(device)\n    # We only extract features, so we don't need gradient\n    with torch.no_grad():\n        # Extract the feature from the image\n        feature = model_yolov8.custom_forward(img)[0].flatten()\n    \n    return feature","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.531590Z","iopub.execute_input":"2023-11-02T15:54:49.532370Z","iopub.status.idle":"2023-11-02T15:54:49.541681Z","shell.execute_reply.started":"2023-11-02T15:54:49.532345Z","shell.execute_reply":"2023-11-02T15:54:49.540727Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# !pip install git+https://github.com/UKPLab/sentence-transformers","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-02T15:54:49.543057Z","iopub.execute_input":"2023-11-02T15:54:49.543407Z","iopub.status.idle":"2023-11-02T15:54:49.553810Z","shell.execute_reply.started":"2023-11-02T15:54:49.543374Z","shell.execute_reply":"2023-11-02T15:54:49.552778Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# from sentence_transformers import SentenceTransformer\n# model_sent_trans = SentenceTransformer('paraphrase-distilroberta-base-v1')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-02T15:54:49.560237Z","iopub.execute_input":"2023-11-02T15:54:49.560565Z","iopub.status.idle":"2023-11-02T15:54:49.565046Z","shell.execute_reply.started":"2023-11-02T15:54:49.560534Z","shell.execute_reply":"2023-11-02T15:54:49.564048Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class HarmemeMemesDatasetAug2(torch.utils.data.Dataset):\n    \"\"\"Uses jsonl data to preprocess and serve \n    dictionary of multimodal tensors for model input.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path,\n        img_dir,\n        split_flag=None,\n        balance=False,\n        dev_limit=None,\n        random_state=0,\n    ):\n\n        self.samples_frame = pd.read_json(\n            data_path, lines=True\n        )\n        self.samples_frame = self.samples_frame.reset_index(\n            drop=True\n        )\n        self.samples_frame.image = self.samples_frame.apply(\n            lambda row: (img_dir + '/' + row.image), axis=1\n        )\n        if split_flag=='train':\n            self.ROI_samples = train_ROI\n            self.ENT_samples = train_ENT\n        elif split_flag=='val':\n            self.ROI_samples = val_ROI\n            self.ENT_samples = val_ENT\n        else:\n            self.ROI_samples = test_ROI\n            self.ENT_samples = test_ENT\n        \n    def __len__(self):\n        \"\"\"This method is called when you do len(instance) \n        for an instance of this class.\n        \"\"\"\n        return len(self.samples_frame)\n\n    def __getitem__(self, idx):\n        \"\"\"This method is called when you do instance[key] \n        for an instance of this class.\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_id = self.samples_frame.loc[idx, \"id\"]\n        img_file_name = self.samples_frame.loc[idx, \"image\"]\n        \n        image_clip_input = process_image_clip(self.samples_frame.loc[idx, \"image\"])\n# --------------------------------------------------------------------------------------        \n#         Pre-extracted features\n#         image_vgg_feature = self.ROI_samples[idx]        \n# --------------------------------------------------------------------------------------\n# On-demand computation\n        BB_info = self.samples_frame.loc[idx, \"bb_dict\"]\n        roi_yolov8_feat_list = []\n        if BB_info:\n            total_BB = len(BB_info)\n            if total_BB>8:\n                BB_info_final = BB_info[:8]\n            else:\n                BB_info_final = BB_info\n#             Have to get VGG reps for each cropped BB and get the mean             \n            for item in BB_info_final:\n#                 Get the top left (left,top) and bottom right (right,bottom) values of the coordinates\n#                 top left and bottom right value extraction                \n                left   = item[0]\n                bottom = item[1]\n                top    = bottom + item[3]\n                right  = left + item[2]\n                #get_image_yolov8_center(img_file_name)   # ??\n                roi_yolov8_feat = get_image_yolov8_BB(left, top, right, bottom, img_file_name)\n                #print(roi_vgg_feat.shape)\n                roi_yolov8_feat_list.append(roi_yolov8_feat)\n#             print(np.shape(roi_vgg_feat_list))\n#             print(torch.cat(roi_vgg_feat_list, dim=0))\n#             print(np.mean(np.array(roi_vgg_feat_list), axis=0))\n            image_yolov8_feature = torch.mean(torch.vstack(roi_yolov8_feat_list), axis=0)\n#             print(image_vgg_feature.shape)\n        else:\n            image_yolov8_feature = torch.tensor(get_image_yolov8_center(img_file_name))\n# --------------------------------------------------------------------------------------\n        text_clip_input = process_text_clip(self.samples_frame.loc[idx, \"text\"])\n#         -------------------------------------------------------------------------------\n#         Process entities\n        #         Use them directly from the saved files\n        text_drob_feature = self.ENT_samples[idx]\n#         -------------------------------------------------------------------------------\n# #         Get the mean representation for the set of entities \"\"on-demand\n#         cur_ent_rep_list = []\n#         cur_ent_list = self.samples_frame.loc[idx, \"ent\"]\n        \n#         if len(cur_ent_list):\n#             for item in cur_ent_list:\n#                 cur_ent_rep = torch.tensor(model_sent_trans.encode(item)).to(device)\n#                 cur_ent_rep_list.append(cur_ent_rep)\n#             text_drob_feature = torch.mean(torch.vstack(cur_ent_rep_list), axis=0)\n#         else:\n#             text_drob_feature = torch.tensor(model_sent_trans.encode(self.samples_frame.loc[idx, \"text\"])).to(device)\n#         -------------------------------------------------------------------------------\n\n        if \"labels\" in self.samples_frame.columns:\n#             label = torch.Tensor(\n#                 [self.samples_frame.loc[idx, \"label\"]]\n#             ).long().squeeze()\n\n#             Uncoment below for binary index creation\n#             if self.samples_frame.loc[idx, \"labels\"][0]==\"not harmful\":\n#                 lab=0\n#             else:\n#                 lab=1            \n#             label = torch.tensor(lab).to(device)  \n\n#             Uncomment below for one hot encoding\n#             y = torch.tensor(lab).to(device)\n#             label = F.one_hot(y, num_classes=2)  \n\n# #             Multiclass setting - harmfulness\n            if self.samples_frame.loc[idx, \"labels\"][0]==\"not harmful\":\n                lab=0\n            elif self.samples_frame.loc[idx, \"labels\"][0]==\"somewhat harmful\":\n                lab=1  \n            else:\n                lab=2\n            label = torch.tensor(lab).to(device)  \n\n            \n            sample = {\n                \"id\": img_id, \n                \"image_clip_input\": image_clip_input,\n                \"image_yolov8_feature\": image_yolov8_feature,\n                \"text_clip_input\": text_clip_input,\n                \"text_drob_embedding\": text_drob_feature,\n                \"label\": label\n            }\n        else:\n            sample = {\n                \"id\": img_id, \n                \"image_clip_input\": image_clip_input,\n                \"image_yolov8_feature\": image_yolov8_feature,\n                \"text_clip_input\": text_clip_input,\n                \"text_drob_embedding\": text_drob_feature\n            }\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.567602Z","iopub.execute_input":"2023-11-02T15:54:49.568001Z","iopub.status.idle":"2023-11-02T15:54:49.587925Z","shell.execute_reply.started":"2023-11-02T15:54:49.567966Z","shell.execute_reply":"2023-11-02T15:54:49.586918Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"hm_dataset_train = HarmemeMemesDatasetAug2(train_path_cov, data_dir_cov, 'train')\ndataloader_train = DataLoader(hm_dataset_train, batch_size=64,\n                        shuffle=True, num_workers=0)\nhm_dataset_val = HarmemeMemesDatasetAug2(dev_path_cov, data_dir_cov, 'val')\ndataloader_val = DataLoader(hm_dataset_val, batch_size=64,\n                        shuffle=True, num_workers=0)\nhm_dataset_test = HarmemeMemesDatasetAug2(test_path_cov, data_dir_cov, 'test')\ndataloader_test = DataLoader(hm_dataset_test, batch_size=64,\n                        shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.589311Z","iopub.execute_input":"2023-11-02T15:54:49.589878Z","iopub.status.idle":"2023-11-02T15:54:49.714130Z","shell.execute_reply.started":"2023-11-02T15:54:49.589851Z","shell.execute_reply":"2023-11-02T15:54:49.713147Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class MM(nn.Module):\n    def __init__(self, n_out):\n        super(MM, self).__init__()  \n        \n        self.dense_yolov8_4096 = nn.Linear(102400, 4096)\n        self.dense_vgg_1024 = nn.Linear(4096, 1024)\n        self.dense_vgg_512 = nn.Linear(1024, 512)\n        self.drop50 = nn.Dropout(p=0.5)\n        self.drop20 = nn.Dropout(p=0.2)\n        self.drop5 = nn.Dropout(p=0.05) \n        \n        self.dense_drob_512 = nn.Linear(768, 512)\n        \n        self.gen_key_L1 = nn.Linear(512, 256) # 512X256\n        self.gen_query_L1 = nn.Linear(512, 256) # 512X256\n        self.gen_key_L2 = nn.Linear(512, 256) # 512X256\n        self.gen_query_L2 = nn.Linear(512, 256) # 512X256\n        self.gen_key_L3 = nn.Linear(512, 256) # 512X256\n        self.gen_query_L3 = nn.Linear(512, 256) # 512X256\n#         self.gen_value = nn.Linear(512, 256) # 512X256\n        self.soft = nn.Softmax(dim=1)\n        self.soft_final = nn.Softmax(dim=1)\n        self.project_dense_512a = nn.Linear(1024, 512) # 512X256\n        self.project_dense_512b = nn.Linear(1024, 512) # 512X256\n        self.project_dense_512c = nn.Linear(1024, 512) # 512X256 \n        \n        \n        self.fc_out = nn.Linear(512, 256) # 512X256\n        self.out = nn.Linear(256, n_out) # 512X256\n        \n\n    def selfattNFuse_L1a(self, vec1, vec2): \n            q1 = F.relu(self.gen_query_L1(vec1))\n            k1 = F.relu(self.gen_key_L1(vec1))\n            q2 = F.relu(self.gen_query_L1(vec2))\n            k2 = F.relu(self.gen_key_L1(vec2))\n            score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n            score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n            wt_score1_score2_mat = torch.cat((score1, score2), 1)\n            wt_i1_i2 = self.soft(wt_score1_score2_mat.float()) #prob\n            prob_1 = wt_i1_i2[:,0]\n            prob_2 = wt_i1_i2[:,1]\n            wtd_i1 = vec1 * prob_1[:, None]\n            wtd_i2 = vec2 * prob_2[:, None]\n            out_rep = F.relu(self.project_dense_512a(torch.cat((wtd_i1,wtd_i2), 1)))\n            return out_rep\n    def selfattNFuse_L1b(self, vec1, vec2): \n            q1 = F.relu(self.gen_query_L2(vec1))\n            k1 = F.relu(self.gen_key_L2(vec1))\n            q2 = F.relu(self.gen_query_L2(vec2))\n            k2 = F.relu(self.gen_key_L2(vec2))\n            score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n            score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n            wt_score1_score2_mat = torch.cat((score1, score2), 1)\n            wt_i1_i2 = self.soft(wt_score1_score2_mat.float()) #prob\n            prob_1 = wt_i1_i2[:,0]\n            prob_2 = wt_i1_i2[:,1]\n            wtd_i1 = vec1 * prob_1[:, None]\n            wtd_i2 = vec2 * prob_2[:, None]\n            out_rep = F.relu(self.project_dense_512b(torch.cat((wtd_i1,wtd_i2), 1)))\n            return out_rep\n    \n    def selfattNFuse_L2(self, vec1, vec2): \n            q1 = F.relu(self.gen_query_L3(vec1))\n            k1 = F.relu(self.gen_key_L3(vec1))\n            q2 = F.relu(self.gen_query_L3(vec2))\n            k2 = F.relu(self.gen_key_L3(vec2))\n            score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n            score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n            wt_score1_score2_mat = torch.cat((score1, score2), 1)\n            wt_i1_i2 = self.soft(wt_score1_score2_mat.float()) #prob\n            prob_1 = wt_i1_i2[:,0]\n            prob_2 = wt_i1_i2[:,1]\n            wtd_i1 = vec1 * prob_1[:, None]\n            wtd_i2 = vec2 * prob_2[:, None]\n            out_rep = F.relu(self.project_dense_512c(torch.cat((wtd_i1,wtd_i2), 1)))\n            return out_rep\n\n\n    def forward(self, in_CI, in_VGG, in_CT, in_Drob):        \n        YOLOV8_feat = self.drop20(F.relu(self.dense_vgg_512(self.drop20(F.relu(self.dense_vgg_1024(self.drop50(F.relu(self.dense_yolov8_4096(in_VGG)))))))))\n        Drob_feat = self.drop5(F.relu(self.dense_drob_512(in_Drob)))\n        out_img = self.selfattNFuse_L1a(YOLOV8_feat, in_CI)\n        out_txt = self.selfattNFuse_L1b(Drob_feat, in_CT)        \n        out_img_txt = self.selfattNFuse_L2(out_img, out_txt)\n        final_out = F.relu(self.fc_out(out_img_txt))\n#         out = torch.sigmoid(self.out(final_out)) #For binary case\n        out = self.out(final_out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.715655Z","iopub.execute_input":"2023-11-02T15:54:49.715926Z","iopub.status.idle":"2023-11-02T15:54:49.741242Z","shell.execute_reply.started":"2023-11-02T15:54:49.715903Z","shell.execute_reply":"2023-11-02T15:54:49.739999Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# output_size = 1 #Binary case\noutput_size = 3\nexp_name = \"EMNLP_MCHarm_GLAREAll_COVTrain_POLEval\"\n# pre_trn_ckp = \"EMNLP_MCHarm_GLAREAll_COVTrain\" # Uncomment for using pre-trained\nexp_path = \"/kaggle/working/EMNLP_ModelCkpt/\"+exp_name\nlr=0.001\n# criterion = nn.BCELoss() #Binary case\ncriterion = nn.CrossEntropyLoss()\n# # ------------Fresh training------------\nmodel = MM(output_size)\nmodel.to(device)\nprint(model)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:49.742845Z","iopub.execute_input":"2023-11-02T15:54:49.743206Z","iopub.status.idle":"2023-11-02T15:54:53.754211Z","shell.execute_reply.started":"2023-11-02T15:54:49.743170Z","shell.execute_reply":"2023-11-02T15:54:53.753213Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"MM(\n  (dense_yolov8_4096): Linear(in_features=102400, out_features=4096, bias=True)\n  (dense_vgg_1024): Linear(in_features=4096, out_features=1024, bias=True)\n  (dense_vgg_512): Linear(in_features=1024, out_features=512, bias=True)\n  (drop50): Dropout(p=0.5, inplace=False)\n  (drop20): Dropout(p=0.2, inplace=False)\n  (drop5): Dropout(p=0.05, inplace=False)\n  (dense_drob_512): Linear(in_features=768, out_features=512, bias=True)\n  (gen_key_L1): Linear(in_features=512, out_features=256, bias=True)\n  (gen_query_L1): Linear(in_features=512, out_features=256, bias=True)\n  (gen_key_L2): Linear(in_features=512, out_features=256, bias=True)\n  (gen_query_L2): Linear(in_features=512, out_features=256, bias=True)\n  (gen_key_L3): Linear(in_features=512, out_features=256, bias=True)\n  (gen_query_L3): Linear(in_features=512, out_features=256, bias=True)\n  (soft): Softmax(dim=1)\n  (soft_final): Softmax(dim=1)\n  (project_dense_512a): Linear(in_features=1024, out_features=512, bias=True)\n  (project_dense_512b): Linear(in_features=1024, out_features=512, bias=True)\n  (project_dense_512c): Linear(in_features=1024, out_features=512, bias=True)\n  (fc_out): Linear(in_features=512, out_features=256, bias=True)\n  (out): Linear(in_features=256, out_features=3, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"! nvcc --version\n! nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:53.755241Z","iopub.execute_input":"2023-11-02T15:54:53.755539Z","iopub.status.idle":"2023-11-02T15:54:55.858860Z","shell.execute_reply.started":"2023-11-02T15:54:53.755513Z","shell.execute_reply":"2023-11-02T15:54:55.857802Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nThu Nov  2 15:54:55 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0    27W /  70W |   2596MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   34C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/Bjarten/early-stopping-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:54:55.860557Z","iopub.execute_input":"2023-11-02T15:54:55.860947Z","iopub.status.idle":"2023-11-02T15:54:58.334089Z","shell.execute_reply.started":"2023-11-02T15:54:55.860914Z","shell.execute_reply":"2023-11-02T15:54:58.332871Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Cloning into 'early-stopping-pytorch'...\nremote: Enumerating objects: 92, done.\u001b[K\nremote: Total 92 (delta 0), reused 0 (delta 0), pack-reused 92\u001b[K\nReceiving objects: 100% (92/92), 533.89 KiB | 3.79 MiB/s, done.\nResolving deltas: 100% (39/39), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -r early-stopping-pytorch/requirements.txt","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-02T15:54:58.335998Z","iopub.execute_input":"2023-11-02T15:54:58.336414Z","iopub.status.idle":"2023-11-02T15:55:10.068350Z","shell.execute_reply.started":"2023-11-02T15:54:58.336375Z","shell.execute_reply":"2023-11-02T15:55:10.067357Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r early-stopping-pytorch/requirements.txt (line 1)) (3.7.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r early-stopping-pytorch/requirements.txt (line 2)) (1.23.5)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r early-stopping-pytorch/requirements.txt (line 3)) (0.15.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (2.8.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (2.31.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (2.0.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r early-stopping-pytorch/requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (2023.7.22)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision->-r early-stopping-pytorch/requirements.txt (line 3)) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp early-stopping-pytorch/pytorchtools.py .","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:55:10.069958Z","iopub.execute_input":"2023-11-02T15:55:10.070368Z","iopub.status.idle":"2023-11-02T15:55:11.067678Z","shell.execute_reply.started":"2023-11-02T15:55:10.070321Z","shell.execute_reply":"2023-11-02T15:55:11.066505Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# import EarlyStopping\nimport os, sys\n#sys.path.append('/opt/conda/lib/python3.10/site-packages/pytorchtools/')\nfrom pytorchtools import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:55:11.069570Z","iopub.execute_input":"2023-11-02T15:55:11.070529Z","iopub.status.idle":"2023-11-02T15:55:11.077529Z","shell.execute_reply.started":"2023-11-02T15:55:11.070485Z","shell.execute_reply":"2023-11-02T15:55:11.076661Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def train_model(model, patience, n_epochs):\n    epochs = n_epochs\n#     clip = 5\n\n    train_acc_list=[]\n    val_acc_list=[]\n    train_loss_list=[]\n    val_loss_list=[]\n    \n        # initialize the experiment path\n    Path(exp_path).mkdir(parents=True, exist_ok=True)\n    # initialize early_stopping object\n    chk_file = os.path.join(exp_path, 'checkpoint_'+exp_name+'.pt')\n    early_stopping = EarlyStopping(patience=patience, verbose=True, path=chk_file)\n\n\n    model.train()\n    for i in range(epochs):\n#         total_acc_train = 0\n        total_loss_train = 0\n        total_train = 0\n        correct_train = 0\n\n        for data in dataloader_train:\n            \n#             Clip features...\n            img_inp_clip = data['image_clip_input']\n            txt_inp_clip = data['text_clip_input']\n            with torch.no_grad():\n                img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n                txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n\n            img_feat_yolov8 = data['image_yolov8_feature']\n            txt_feat_trans = data['text_drob_embedding']\n\n            label_train = data['label'].to(device)\n\n            model.zero_grad()\n            \n            output = model(img_feat_clip, img_feat_yolov8, txt_feat_clip, txt_feat_trans)\n#             print(output.shape)\n#             output = model(img_feat_vgg, txt_feat_trans)\n\n            loss = criterion(output.squeeze(), label_train)\n            \n#             print(loss)\n            loss.backward()\n#             nn.utils.clip_grad_norm_(model.parameters(), clip)\n            optimizer.step()\n\n            with torch.no_grad():\n                _, predicted_train = torch.max(output.data, 1)\n                total_train += label_train.size(0)\n                correct_train += (predicted_train == label_train).sum().item()\n#                 out_val = (output.squeeze()>0.5).float()\n#                 out_final = ((out_val == 1).nonzero(as_tuple=True)[0])\n#                 print()\n#                 acc = torch.abs(output.squeeze() - label.float()).view(-1)\n#                 acc = (1. - acc.sum() / acc.size()[0])\n#                 total_acc_train += acc\n                total_loss_train += loss.item()\n\n        \n        train_acc = 100 * correct_train / total_train\n        train_loss = total_loss_train/total_train\n        model.eval()\n#         total_acc_val = 0\n        total_loss_val = 0\n        total_val = 0\n        correct_val = 0\n\n        with torch.no_grad():\n            for data in dataloader_val:                \n#                 Clip features...                \n                img_inp_clip = data['image_clip_input']\n                txt_inp_clip = data['text_clip_input']\n                with torch.no_grad():\n                    img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n                    txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n                \n                \n                img_feat_yolov8 = data['image_yolov8_feature']                \n                txt_feat_trans = data['text_drob_embedding']\n\n                \n\n                label_val = data['label'].to(device)\n\n                model.zero_grad()\n                \n                output = model(img_feat_clip, img_feat_yolov8, txt_feat_clip, txt_feat_trans)\n#                 output = model(img_feat_vgg, txt_feat_trans)\n                \n                \n                val_loss = criterion(output.squeeze(), label_val)\n                _, predicted_val = torch.max(output.data, 1)\n                total_val += label_val.size(0)\n                correct_val += (predicted_val == label_val).sum().item()                \n                total_loss_val += val_loss.item()\n        print(\"Saving model...\") \n        torch.save(model.state_dict(), os.path.join(exp_path, \"final.pt\"))\n\n        val_acc = 100 * correct_val / total_val\n        val_loss = total_loss_val/total_val\n\n        train_acc_list.append(train_acc)\n        val_acc_list.append(val_acc)\n        train_loss_list.append(train_loss)\n        val_loss_list.append(val_loss)\n        \n        early_stopping(val_loss, model)\n        \n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n            \n        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n        model.train()\n        torch.cuda.empty_cache()\n        \n    # load the last checkpoint with the best model\n#     model.load_state_dict(torch.load('checkpoint_1.pt'))\n    \n    return  model, train_acc_list, val_acc_list, train_loss_list, val_loss_list, i","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:55:11.079125Z","iopub.execute_input":"2023-11-02T15:55:11.079486Z","iopub.status.idle":"2023-11-02T15:55:11.101153Z","shell.execute_reply.started":"2023-11-02T15:55:11.079458Z","shell.execute_reply":"2023-11-02T15:55:11.100197Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def test_model(model):\n    model.eval()\n    total_test = 0\n    correct_test =0\n    total_acc_test = 0\n    total_loss_test = 0\n    outputs = []\n    test_labels=[]\n    with torch.no_grad():\n        for data in dataloader_test:\n            img_inp_clip = data['image_clip_input']\n            txt_inp_clip = data['text_clip_input']\n            with torch.no_grad():\n                img_feat_clip = clip_model.encode_image(img_inp_clip).float().to(device)\n                txt_feat_clip = clip_model.encode_text(txt_inp_clip).float().to(device)\n\n            img_feat_yolov8 = data['image_yolov8_feature']\n            txt_feat_trans = data['text_drob_embedding']            \n\n            label_test = data['label'].to(device)\n            \n#             out = model(img_feat_vgg, txt_feat_trans)        \n\n            out = model(img_feat_clip, img_feat_yolov8, txt_feat_clip, txt_feat_trans)        \n\n            outputs += list(out.cpu().data.numpy())\n            loss = criterion(out.squeeze(), label_test)\n            \n            _, predicted_test = torch.max(out.data, 1)\n            total_test += label_test.size(0)\n            correct_test += (predicted_test == label_test).sum().item()\n#                 out_val = (output.squeeze()>0.5).float()\n#                 out_final = ((out_val == 1).nonzero(as_tuple=True)[0])\n#                 print()\n#                 acc = torch.abs(output.squeeze() - label.float()).view(-1)\n#                 acc = (1. - acc.sum() / acc.size()[0])\n#                 total_acc_train += acc\n            total_loss_test += loss.item()\n            \n            \n#     #         print(label.float())\n#             acc = torch.abs(out.squeeze() - label.float()).view(-1)\n#     #         print((acc.sum() / acc.size()[0]))\n#             acc = (1. - acc.sum() / acc.size()[0])\n#     #         print(acc)\n#             total_acc_test += acc\n#             total_loss_test += loss.item()\n\n    \n    acc_test = 100 * correct_test / total_test\n    loss_test = total_loss_test/total_test   \n    \n    print(f'acc: {acc_test:.4f} loss: {loss_test:.4f}')\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:55:11.102686Z","iopub.execute_input":"2023-11-02T15:55:11.102984Z","iopub.status.idle":"2023-11-02T15:55:11.115943Z","shell.execute_reply.started":"2023-11-02T15:55:11.102957Z","shell.execute_reply":"2023-11-02T15:55:11.114926Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:55:11.117213Z","iopub.execute_input":"2023-11-02T15:55:11.118145Z","iopub.status.idle":"2023-11-02T15:55:11.128929Z","shell.execute_reply.started":"2023-11-02T15:55:11.118108Z","shell.execute_reply":"2023-11-02T15:55:11.128109Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"epoc_num_total = 0\ntrain_acc_list_full = []\nval_acc_list_full = []\ntrain_loss_list_full = []\nval_loss_list_full = []","metadata":{"execution":{"iopub.status.busy":"2023-11-02T15:55:11.130247Z","iopub.execute_input":"2023-11-02T15:55:11.130609Z","iopub.status.idle":"2023-11-02T15:55:11.139172Z","shell.execute_reply.started":"2023-11-02T15:55:11.130573Z","shell.execute_reply":"2023-11-02T15:55:11.138342Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"n_epochs = 3\n# early stopping patience; how long to wait after last time validation loss improved.\npatience = 1\nmodel, train_acc_list, val_acc_list, train_loss_list, val_loss_list, epoc_num = train_model(model, patience, n_epochs)\nepoc_num_total += epoc_num\ntrain_acc_list_full += train_acc_list\nval_acc_list_full += val_acc_list\ntrain_loss_list_full += train_loss_list\nval_loss_list_full += val_loss_list","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-02T15:55:11.140146Z","iopub.execute_input":"2023-11-02T15:55:11.140425Z","iopub.status.idle":"2023-11-02T16:02:29.522866Z","shell.execute_reply.started":"2023-11-02T15:55:11.140400Z","shell.execute_reply":"2023-11-02T16:02:29.521566Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"Saving model...\nValidation loss decreased (inf --> 0.011006).  Saving model ...\nEpoch 1: train_loss: 0.0113 train_acc: 69.7975 | val_loss: 0.0110 val_acc: 75.7062\n","output_type":"stream"},{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"Saving model...\nEarlyStopping counter: 1 out of 1\nEarly stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# get_ipython().run_line_magic('matplotlib', 'inline')\n# epochs = range(epoc_num_total+1)\n# train_acc_list\n# val_acc_list\n# train_loss_list\n# val_loss_list\n# # plt.plot(epochs, train_acc_list)\n# # plt.plot(epochs, val_acc_list)\n# fig1, ax1 = plt.subplots()\n# ax1.plot(epochs, train_acc_list_full, label=\"train acc\")\n# ax1.plot(epochs, val_acc_list_full, label=\"val acc\")\n# ax1.set_title(\"accuracy plot\")\n# ax1.set_xlabel(\"epochs\")\n# ax1.legend(loc=\"upper left\")\n# fig2, ax2 = plt.subplots()\n# ax2.plot(epochs, train_loss_list_full, label=\"train loss\")\n# ax2.plot(epochs, val_loss_list_full, label=\"val loss\")\n# ax2.set_title(\"loss plot\")\n# ax2.set_xlabel(\"epochs\")\n# ax2.legend(loc=\"upper left\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T16:02:29.524291Z","iopub.execute_input":"2023-11-02T16:02:29.524725Z","iopub.status.idle":"2023-11-02T16:02:29.530095Z","shell.execute_reply.started":"2023-11-02T16:02:29.524685Z","shell.execute_reply":"2023-11-02T16:02:29.529172Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"outputs = test_model(model)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T16:02:29.531301Z","iopub.execute_input":"2023-11-02T16:02:29.531586Z","iopub.status.idle":"2023-11-02T16:02:50.719188Z","shell.execute_reply.started":"2023-11-02T16:02:29.531563Z","shell.execute_reply":"2023-11-02T16:02:50.718261Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"acc: 79.6610 loss: 0.0094\n","output_type":"stream"},{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred=[]\nfor i in outputs:\n#     print(np.argmax(i))\n    y_pred.append(np.argmax(i))\n# # np.argmax(outputs[:])\n# outputs\n\n# # Multiclass setting\ntest_labels=[]\nfor index, row in test_samples_frame.iterrows():\n    lab = row['labels'][0]\n    if lab==\"not harmful\":\n        test_labels.append(0)\n    elif lab==\"somewhat harmful\":\n        test_labels.append(1)\n    else:\n        test_labels.append(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T16:02:50.720395Z","iopub.execute_input":"2023-11-02T16:02:50.720729Z","iopub.status.idle":"2023-11-02T16:02:50.754977Z","shell.execute_reply.started":"2023-11-02T16:02:50.720703Z","shell.execute_reply":"2023-11-02T16:02:50.754280Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def calculate_mmae(expected, predicted, classes):\n    NUM_CLASSES = len(classes)\n    count_dict = {}\n    dist_dict = {}\n    for i in range(NUM_CLASSES):\n        count_dict[i] = 0\n        dist_dict[i] = 0.0\n    for i in range(len(expected)):\n        dist_dict[expected[i]] += abs(expected[i] - predicted[i])\n        count_dict[expected[i]] += 1\n    overall = 0.0\n    for claz in range(NUM_CLASSES): \n        class_dist =  1.0 * dist_dict[claz] / count_dict[claz] \n        overall += class_dist\n    overall /= NUM_CLASSES\n#     return overall[0]\n    return overall","metadata":{"execution":{"iopub.status.busy":"2023-11-02T16:02:50.756147Z","iopub.execute_input":"2023-11-02T16:02:50.756477Z","iopub.status.idle":"2023-11-02T16:02:50.763335Z","shell.execute_reply.started":"2023-11-02T16:02:50.756445Z","shell.execute_reply":"2023-11-02T16:02:50.762430Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"rec = np.round(recall_score(test_labels, y_pred, average=\"macro\"),4)\nprec = np.round(precision_score(test_labels, y_pred, average=\"macro\"),4)\nf1 = np.round(f1_score(test_labels, y_pred, average=\"macro\"),4)\n# hl = np.round(hamming_loss(test_labels, y_pred),4)\nacc = np.round(accuracy_score(test_labels, y_pred),4)\nmmae = np.round(calculate_mmae(test_labels, y_pred, [0,1,2]),4)\nmae = np.round(mean_absolute_error(test_labels, y_pred),4)\n# print(\"recall_score\\t: \",rec)\n# print(\"precision_score\\t: \",prec)\n# print(\"f1_score\\t: \",f1)\n# print(\"hamming_loss\\t: \",hl)\n# print(\"accuracy_score\\t: \",f1)\nprint(classification_report(test_labels, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-11-02T16:02:50.764827Z","iopub.execute_input":"2023-11-02T16:02:50.765206Z","iopub.status.idle":"2023-11-02T16:02:50.796431Z","shell.execute_reply.started":"2023-11-02T16:02:50.765173Z","shell.execute_reply":"2023-11-02T16:02:50.795538Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.90      0.86      0.88       230\n           1       0.62      0.83      0.71       103\n           2       0.00      0.00      0.00        21\n\n    accuracy                           0.80       354\n   macro avg       0.51      0.56      0.53       354\nweighted avg       0.77      0.80      0.78       354\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Acc, F1, Rec, Prec, MAE, MMAE\")\nprint(acc, f1, rec, prec, mae, mmae)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T16:02:50.797582Z","iopub.execute_input":"2023-11-02T16:02:50.797970Z","iopub.status.idle":"2023-11-02T16:02:50.803347Z","shell.execute_reply.started":"2023-11-02T16:02:50.797935Z","shell.execute_reply":"2023-11-02T16:02:50.802393Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Acc, F1, Rec, Prec, MAE, MMAE\n0.7966 0.5303 0.5606 0.5096 0.2119 0.487\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}